{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial step: build dictionary of `{year:mainurl}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.letour.fr/en/history')\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "results = soup.find_all('button') #class=\"dateTabs__link js-tabs\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_links_dict = {}\n",
    "\n",
    "for item in results:\n",
    "    if item.text:\n",
    "        key = int(item.text)\n",
    "        value = {'mainurl':item['data-tabs-ajax']}\n",
    "        year_links_dict[key] = value\n",
    "        \n",
    "print(len(year_links_dict))\n",
    "year_links_dict#[2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for year, dct in year_links_dict.items():\n",
    "    print(year, dct['mainurl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding links to `Starters`, `Stages`, `Jersey wearers`, `Stage winners` and `Ranking` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "for year in list(year_links_dict.keys()):\n",
    "    print('Main URL: ', year, year_links_dict[year]['mainurl'])\n",
    "    r = requests.get(base_url + year_links_dict[year]['mainurl'])\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    new_dict = {}\n",
    "    buttons = soup.find_all('button', class_=\"js-tabs-nested\")\n",
    "    for b in buttons:\n",
    "        new_key = b.text.lower().replace(' ', '_') + '_url'\n",
    "        new_val = b['data-tabs-ajax']\n",
    "        year_links_dict[year][new_key] = new_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(obj, filename):\n",
    "    with open('data/' + filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/year_links_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(year_links_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open from `pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(name):\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_links_dict = load_pickle('year_links_dict')\n",
    "year_links_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add `num_of_stages` to dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrape the total number of stages, for a particular year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "def scrape_number_of_stages(year):\n",
    "    print(f'Scraping number of stages for {year}')\n",
    "    r = requests.get(base_url + year_links_num_dict[year]['mainurl'])\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return int(soup.find_all('span', class_='statsInfos__number')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainurl = '/en/block/history/11818/f34c3404d95a697dcf77d4cd8e8278fa' # 2018 - 21 stages\n",
    "# mainurl = '/en/block/history/10804/89b36b01ffe439e016ec1d59c57b63d1' # 2011 - 21 stages\n",
    "mainurl = '/en/block/history/10708/0b76b8f809ad5d8bcf3579df597644d8'  # 1904 - 6 stages\n",
    "\n",
    "print(scrape_number_of_stages(1956))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year_links_num_dict = year_links_dict.copy()\n",
    "\n",
    "for year in list(year_links_num_dict.keys()):\n",
    "    year_links_num_dict[year]['num_of_stages'] = scrape_number_of_stages(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(year_links_num_dict, 'year_links_num_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load `year_links_num_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_links_num_dict = load_pickle('year_links_num_dict')\n",
    "year_links_num_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to scrape data from different tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "def scrape_starters(year):\n",
    "    print(f'Scraping starters for {year}')\n",
    "    r = requests.get(base_url + year_links_num_dict[year]['starters_url'], timeout=10)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    rows = soup.find_all('tr')\n",
    "    rows_list = []\n",
    "\n",
    "    for i, row in enumerate(rows):\n",
    "        rows_list.append(row.text.strip())\n",
    "        \n",
    "    starters_list = []\n",
    "\n",
    "    for row in rows_list:\n",
    "        matched_on_team = re.search(r'^[a-zA-Z]', row)\n",
    "\n",
    "        if matched_on_team: # if row is a team name\n",
    "            new_row = [row,None,None]\n",
    "            starters_list.append(new_row)\n",
    "        else: # row is a rider\n",
    "            new_row = [None]\n",
    "            new_row.append([x.strip() for x in row.split('\\n ')][0])\n",
    "            new_row.append([x.strip() for x in row.split('\\n ')][1])\n",
    "            starters_list.append(new_row)\n",
    "            \n",
    "    starters_df = pd.DataFrame(starters_list, columns=['team','rider_num','rider_name'])\n",
    "    starters_df.team = starters_df.team.fillna(method='ffill')\n",
    "    starters_df = starters_df[starters_df.rider_num.isnull() == False]\n",
    "    starters_df = starters_df.reset_index(drop=True)\n",
    "    filepath = 'data/' + str(year) + '/' + str(year) + '_starters' + '.csv'\n",
    "    starters_df.to_csv(filepath, index=False)\n",
    "    print('Saved ' + filepath)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "def scrape_stages(year):\n",
    "    print(f'Scraping stages for {year}')\n",
    "    r = requests.get(base_url + year_links_num_dict[year]['stages_url'], timeout=10)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    stages_list = []\n",
    "    rows = soup.find_all('tr')\n",
    "    for i, row in enumerate(rows):\n",
    "        stages_list.append(row.text.strip())\n",
    "\n",
    "    stages_list = [i.split('\\n') for i in stages_list]\n",
    "    stages_list.pop(0)\n",
    "    header = ['stage_num','date_start','start_city','finish_city']\n",
    "    stages_df = pd.DataFrame(stages_list, columns=header)\n",
    "    filepath = 'data/' + str(year) + '/' + str(year) + '_stages' + '.csv'\n",
    "    stages_df.to_csv(filepath, index=False)\n",
    "    print('Saved ' + filepath)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Jersey wearers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "def scrape_jersey_wearers(year):\n",
    "    print(f'Scraping jersey wearers for {year}')\n",
    "    r = requests.get(base_url + year_links_num_dict[year]['jersey_wearers_url'], timeout=10)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    jersey_list = []\n",
    "    num_of_cols = 5\n",
    "    \n",
    "    rows = soup.find_all('tr')\n",
    "    for row in rows:\n",
    "        new_row = row.text.strip('\\n')\n",
    "        new_row = re.sub(r'\\s\\s+',',',new_row)\n",
    "        #new_row = re.sub(r',$','',new_row) # commented out to prevent column number not matching issue\n",
    "        jersey_list.append(new_row)\n",
    "\n",
    "    jersey_list = [i.split(',') for i in jersey_list]\n",
    "    header = jersey_list.pop(0)\n",
    "    header = [x.lower().replace(' ', '_') for x in header[0].split('\\n')] # modified header code to fix col match bug\n",
    "    header[0] = 'stage_num'\n",
    "    #header = ['stage_num','yellow_jersey','green_jersey','polka_dot_jersey','white_jersey']\n",
    "    jersey_df = pd.DataFrame(jersey_list)\n",
    "    \n",
    "    while len(header) > len(jersey_df.columns): # while header is longer than num of cols in df\n",
    "        header.pop()\n",
    "    \n",
    "    while len(jersey_df.columns) > num_of_cols: # while df has more columns than it should (fixes column number not matching bug)\n",
    "        jersey_df = jersey_df.drop(jersey_df.columns[len(jersey_df.columns)-1], axis=1)\n",
    "    \n",
    "    jersey_df.columns = header\n",
    "\n",
    "    filepath = 'data/' + str(year) + '/' + str(year) + '_jersey_wearers' + '.csv'\n",
    "    jersey_df.to_csv(filepath, index=False)\n",
    "    print('Saved ' + filepath)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stage winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "def scrape_stage_winners(year):\n",
    "    print(f'Scraping stage winners for {year}')\n",
    "    r = requests.get(base_url + year_links_num_dict[year]['stages_winners_url'], timeout=10)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    stage_winners_list = []\n",
    "\n",
    "    rows = soup.find_all('tr')\n",
    "    for row in rows:\n",
    "        new_row = row.text.strip('\\n')\n",
    "        new_row = re.sub(r'\\s\\s\\s+',',',new_row) # added a third \\s to fix bug of splitting on team name in parens\n",
    "        new_row = re.sub(r',$','',new_row)\n",
    "        new_row = re.sub(r'\\n',',',new_row)\n",
    "        stage_winners_list.append(new_row)\n",
    "\n",
    "    stage_winners_list = [i.split(',') for i in stage_winners_list]\n",
    "    stage_winners_list.pop(0)\n",
    "    header = ['stage_num','parcours','winner','team']\n",
    "    stage_winners_df = pd.DataFrame(stage_winners_list\n",
    "                                    , columns=header\n",
    "                                   )\n",
    "    filepath = 'data/' + str(year) + '/' + str(year) + '_stage_winners' + '.csv'\n",
    "    stage_winners_df.to_csv(filepath, index=False)\n",
    "    print('Saved ' + filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Rankings per stage (i.e. times, gaps, points, etc.)\n",
    "What are `b` and `p`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to loop through all ranking codes for a given year, and scrape/export data as CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_cats = {'indiv_general':'itg',\n",
    "                'indiv_stage':'ite',\n",
    "                'points_general':'ipg',\n",
    "                #'points_stage':'ipe',\n",
    "                'climber_general':'img',\n",
    "                #'climber_stage':'ime',\n",
    "                'youth_general':'ijg',\n",
    "                #'combative_general':'icg',\n",
    "                'team_stage':'ete',\n",
    "                'team_general':'etg'\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "def scrape_all_rankings(year):\n",
    "    print(f'Scraping rankings for all codes for {year}')\n",
    "    \n",
    "    for label, code in ranking_cats.items(): # loop through ranking codes\n",
    "    \n",
    "        print(label, code)\n",
    "\n",
    "        num_columns = -1 # added this code block to fix header length not matching number of cols of data bug\n",
    "        if code == 'itg' or code == 'ite':\n",
    "#             continue\n",
    "            header = ['rank','rider','rider_no','team','times','gap','b','p']\n",
    "            num_columns = 8\n",
    "        elif code == 'ipg':\n",
    "#             continue\n",
    "            header = ['rank','rider','rider_no','team','points','b','p']\n",
    "            num_columns = 7\n",
    "        elif code == 'img':\n",
    "            header = ['rank','rider','rider_no','team','points']\n",
    "            num_columns = 5\n",
    "        elif code == 'ijg':\n",
    "#             continue\n",
    "            header = ['rank','rider','rider_no','team','times','gap']\n",
    "            num_columns = 6\n",
    "        elif code == 'ete' or code == 'etg':\n",
    "#             continue\n",
    "            header = ['rank','team','times','gap']\n",
    "            num_columns = 4\n",
    "        else:\n",
    "            print('Not a ranking code I am interested in!')\n",
    "\n",
    "        rankings_df = pd.DataFrame()\n",
    "\n",
    "        for stage_num in range(year_links_num_dict[year]['num_of_stages']): # loop through stages\n",
    "            stage_num += 1\n",
    "            print('\\nStage number:', stage_num)\n",
    "            full_url = str(base_url + year_links_num_dict[year]['ranking_url'] \n",
    "                         + f\"?stage={stage_num}\" \n",
    "                         + f\"&type={code}\")\n",
    "            r = requests.get(full_url, timeout=None)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            print(full_url)\n",
    "\n",
    "            rows_for_df = {} \n",
    "            row_num = 0\n",
    "            for item in soup.tbody.find_all('tr'):\n",
    "                row = item.find_all('td')\n",
    "\n",
    "                if len(row) == num_columns: # check for if number of columns in header matches\n",
    "\n",
    "                    new_row = []\n",
    "                    for col in row: \n",
    "                        new_row.append(col.text.strip())\n",
    "                        rows_for_df[row_num] = new_row\n",
    "                    row_num += 1\n",
    "               \n",
    "            df = pd.DataFrame.from_dict(rows_for_df, orient='index'\n",
    "                                    , columns=header\n",
    "                                       )\n",
    "\n",
    "            df['stage_num'] = stage_num\n",
    "            cols = list(df.columns)\n",
    "            cols = [cols[-1]] + cols[:-1]\n",
    "            df = df[cols]\n",
    "            df = df.reset_index(drop=True)\n",
    "\n",
    "            rankings_df = pd.concat([rankings_df, df])\n",
    "                                 \n",
    "        filepath = 'data/' + str(year) + '/' + str(year) + '_rankings_' + code + '.csv'\n",
    "        rankings_df.to_csv(filepath, index=False)\n",
    "        print('Saved ' + filepath)\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Scrape! Loop through all years..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished years: \n",
    "\n",
    "`[\n",
    "1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,\n",
    "2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,\n",
    "2010,2011,2012,2013,2014,2015,2016,2017,2018\n",
    "]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-1966+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(reversed(list(year_links_num_dict.keys())))[53:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://www.letour.fr'\n",
    "\n",
    "for year in list(reversed(list(year_links_num_dict.keys())))[53:]: # starting from 1965, going backwards\n",
    "    directory = 'data/' + str(year)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    scrape_all_rankings(year) # 1. scrape rankings data\n",
    "    time.sleep(10)\n",
    "    scrape_starters(year) # 2. scrape starters data\n",
    "    time.sleep(10)\n",
    "    scrape_stages(year) # 3. scrape stages data\n",
    "    time.sleep(10)\n",
    "    scrape_jersey_wearers(year) # 4. scrape jersey wearers data\n",
    "    time.sleep(10)\n",
    "    scrape_stage_winners(year) # 5. scrape stage winners data\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
